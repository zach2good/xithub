# This script processes the packet files in the 'packets' directory.
# It reads the packet files, processes them, and moves them to the 'processed_packets' directory.
# It also maintains a session file that contains the user sessions.
# A session is created for each user and updated whenever a packet is processed.
# The session file is used to track the user sessions and cut the sessions that are older than a certain time.
#
# Processing includes collecting the following information, for insertion into the database:
# - RowId: Autoincremented Row ID
# - UserId (ClientToken): The website-generated ID of the submitting user, we'll match the incoming IP with this to make sure they match
# - SessionId: Generated by us: We'll cut sessions after 5 minutes of inactivity. We'll make session cuts requestable by users in the future.
# - CharacterName: The logged in character's name.
# - ServerId: The retail server the character is on.
# - ZoneId: The zone of the logged in character.
# - ServerTimestamp: When the server recieved this packet in a collection with other packets
# - ClientTimestamp: When the client recieved or sent this packet and it was copied for us to use for our purposes.
# - PacketId: The 9-byte ID of the packet.
# - RawData: A BLOB of the original packet.
import os
import shutil
import time
import struct
import gzip
import hashlib
import mysql.connector # pip3 install mysql-connector-python, TODO: requirements file
from datetime import datetime, timedelta

# Configuration
INPUT_DIR = 'packets'
PROCESSED_DIR = 'processed_packets'
PROCESS_INTERVAL = 10  # 10 seconds
RUNTIME_DATA_DIR = 'runtime_data'
SESSIONS_FILE = os.path.join(RUNTIME_DATA_DIR, 'sessions.txt')
SESSION_MAX_AGE_MINUTES = 2

# Database Configuration (TODO: env vars, or whatever)
DB_HOST = 'localhost'
DB_USER = 'root'
DB_PASSWORD = 'root'
DB_NAME = 'testdb'


def db_connect():
    return mysql.connector.connect(
        host=DB_HOST,
        user=DB_USER,
        password=DB_PASSWORD,
        database=DB_NAME
    )


def generate_session_id(user_id):
    return str(user_id) + str(int(time.time()))


def tap_session(user_id):
    session_exists = False
    session_id = None
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    sessions = []

    if os.path.exists(SESSIONS_FILE):
        with open(SESSIONS_FILE, 'r') as f:
            sessions = f.readlines()

    for i, session in enumerate(sessions):
        parts = session.strip().split(',')
        if parts[0] == str(user_id):
            session_id = parts[1]
            sessions[i] = f"{user_id},{session_id},{parts[2]},{current_time}\n"
            session_exists = True
            break

    if not session_exists:
        session_id = generate_session_id(user_id)
        sessions.append(f"{user_id},{session_id},{current_time},{current_time}\n")

    with open(SESSIONS_FILE, 'w') as f:
        f.writelines(sessions)


def try_cut_sessions():
    current_time = datetime.now()
    sessions = []

    if os.path.exists(SESSIONS_FILE):
        with open(SESSIONS_FILE, 'r') as f:
            sessions = f.readlines()

    updated_sessions = []
    for session in sessions:
        parts = session.strip().split(',')
        last_updated_time = datetime.strptime(parts[3], '%Y-%m-%d %H:%M:%S')
        if current_time - last_updated_time <= timedelta(minutes=SESSION_MAX_AGE_MINUTES):
            updated_sessions.append(session)

    with open(SESSIONS_FILE, 'w') as f:
        f.writelines(updated_sessions)

def get_user_session_id_from_user_id(user_id):
    if os.path.exists(SESSIONS_FILE):
        with open(SESSIONS_FILE, 'r') as f:
            sessions = f.readlines()
            for session in sessions:
                parts = session.strip().split(',')
                if parts[0] == str(user_id):
                    return parts[1]
    return None


def process_packet(file_path, db_connection):
    print(f"Processing file: {file_path}")

    userId = get_user_id_from_filename(file_path)
    tap_session(userId)

    with open(file_path, 'rb') as f:
        data = f.read()

        # Define the outer header format
        outer_header_format = '<QQB15sI'
        outer_header_size = struct.calcsize(outer_header_format)

        # Unpack the outer header
        outer_header_data = data[:outer_header_size]
        clientToken, outer_timestamp, serverId, characterName, dataSize = struct.unpack(outer_header_format, outer_header_data)

        characterName = characterName.decode('utf-8', errors='ignore').rstrip('\x00')

        # Extract the compressed inner data
        compressed_data = data[outer_header_size:outer_header_size + dataSize]

        # Decompress the inner data
        inner_data = gzip.decompress(compressed_data)

        # Define the inner header format
        inner_header_format = '<QHI'
        inner_header_size = struct.calcsize(inner_header_format)

        # Unpack the inner header
        inner_header_data = inner_data[:inner_header_size]
        inner_timestamp, zoneId, inner_dataSize = struct.unpack(inner_header_format, inner_header_data)

        offset = 0

        # Loop through the inner data to extract multiple inner packets
        while offset < len(inner_data):
            if len(inner_data) - offset < inner_header_size:
                print("Error: Remaining data is smaller than inner header size.")
                break

            # Unpack the inner header
            inner_header_data = inner_data[offset:offset + inner_header_size]
            inner_timestamp, zoneId, inner_dataSize = struct.unpack(inner_header_format, inner_header_data)

            offset += inner_header_size

            if len(inner_data) - offset < inner_dataSize:
                print("Error: Remaining data is smaller than specified inner data size.")
                break

            # Extract the actual packet data
            packet_data = inner_data[offset:offset + inner_dataSize]

            offset += inner_dataSize

            sessionId = get_user_session_id_from_user_id(clientToken)
            packetId  = hex(packet_data[0] & 0x1FF)

            # Create a hash of the packet_data to use as a unique identifier
            packet_hash = hashlib.sha256(packet_data).hexdigest()

            # DEBUG
            # Print all data as single line
            # print(f"Processing packet: {packetId} from {characterName} in zone {zoneId} at {inner_timestamp} with session ID {sessionId} and hash {packet_hash}")
            # print(f"Raw data: {packet_data}")

            # Insert the packet data into the database
            cursor = db_connection.cursor()
            insert_query = """
                INSERT INTO packets (
                    UserId, SessionId, CharacterName, ServerId, ZoneId, ServerTimestamp, ClientTimestamp, PacketId, PacketHash, RawData
                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """
            try:
                cursor.execute(insert_query, (
                    clientToken, sessionId, characterName, serverId, zoneId, outer_timestamp, inner_timestamp, packetId, packet_hash, packet_data
                ))
                db_connection.commit()
                cursor.close()
            except mysql.connector.Error as err:
                print(f"Error inserting packet data: {err}")
                cursor.close()

    print(f"Finished processing file: {file_path}")


def move_to_processed(file_path, processed_dir):
    if not os.path.exists(processed_dir):
        os.makedirs(processed_dir)

    filename = os.path.basename(file_path)
    dest_path = os.path.join(processed_dir, filename)
    shutil.move(file_path, dest_path)
    print(f"Moved {file_path} to {dest_path}")


def get_timestamp_from_filename(filename):
    try:
        # filename format is "packet_userId_timestamp.bin"
        parts = filename.split('_')
        timestamp = int(parts[2].split('.')[0])
        return timestamp
    except (IndexError, ValueError):
        return None


def get_user_id_from_filename(filename):
    try:
        # filename format is "packet_userId_timestamp.bin"
        parts = filename.split('_')
        user_id = int(parts[1])
        return user_id
    except (IndexError, ValueError):
        return None


def process_packets(db_connection):
    if not os.path.exists(INPUT_DIR):
        print(f"Input directory {INPUT_DIR} does not exist.")
        return

    # .bin files
    packet_files = [f for f in os.listdir(INPUT_DIR) if os.path.isfile(os.path.join(INPUT_DIR, f)) and f.endswith('.bin')]

    if not packet_files:
        print(f"No packet files found in {INPUT_DIR}.")
        return

    # Sort packet files based on the timestamp extracted from filenames
    packet_files.sort(key=get_timestamp_from_filename)

    for packet_file in packet_files:
        packet_file_path = os.path.join(INPUT_DIR, packet_file)
        process_packet(packet_file_path, db_connection)
        move_to_processed(packet_file_path, PROCESSED_DIR)


if __name__ == '__main__':
    while True:
        start = datetime.now()
        print(f"Packet processing started at {start}")

        db_connection = db_connect()
        process_packets(db_connection)
        db_connection.close()

        try_cut_sessions()

        end = datetime.now()
        print(f"Packet processing completed at {end}")
        print(f"Total processing time: {end - start}")

        next_run = end + timedelta(seconds=PROCESS_INTERVAL)
        print(f"Next run will be at {next_run}")
        time.sleep(PROCESS_INTERVAL)
